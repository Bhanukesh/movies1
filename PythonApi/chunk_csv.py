#!/usr/bin/env python3
"""
Script to chunk the large CSV file into smaller, manageable pieces
"""
import pandas as pd
import os
import math
from pathlib import Path

def chunk_csv_file(input_file: str, chunk_size: int = 1000, output_dir: str = "data_chunks"):
    """
    Split a large CSV file into smaller chunks
    
    Args:
        input_file: Path to the input CSV file
        chunk_size: Number of rows per chunk
        output_dir: Directory to store chunks
    """
    input_path = Path(input_file)
    
    if not input_path.exists():
        print(f"âŒ Input file not found: {input_file}")
        return False
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    print(f"ğŸ“‚ Processing: {input_file}")
    print(f"ğŸ“Š Chunk size: {chunk_size} rows")
    print(f"ğŸ’¾ Output directory: {output_dir}")
    
    try:
        # Read the CSV in chunks to avoid memory issues
        chunk_reader = pd.read_csv(
            input_path,
            encoding='latin-1',
            chunksize=chunk_size,
            on_bad_lines='skip',
            low_memory=False
        )
        
        chunk_number = 0
        total_rows = 0
        
        for chunk_df in chunk_reader:
            chunk_number += 1
            chunk_filename = f"movies_chunk_{chunk_number:03d}.csv"
            chunk_path = output_path / chunk_filename
            
            # Save chunk
            chunk_df.to_csv(chunk_path, index=False)
            total_rows += len(chunk_df)
            
            print(f"âœ… Created {chunk_filename}: {len(chunk_df)} rows")
        
        print(f"\nğŸ‰ Chunking completed!")
        print(f"ğŸ“ˆ Total rows processed: {total_rows}")
        print(f"ğŸ“¦ Total chunks created: {chunk_number}")
        print(f"ğŸ’½ Average chunk size: {total_rows // chunk_number} rows")
        
        # Create a metadata file
        metadata = {
            "original_file": str(input_path.name),
            "total_chunks": chunk_number,
            "total_rows": total_rows,
            "chunk_size": chunk_size,
            "chunk_pattern": "movies_chunk_*.csv"
        }
        
        metadata_path = output_path / "metadata.json"
        import json
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"ğŸ“‹ Metadata saved to: {metadata_path}")
        return True
        
    except Exception as e:
        print(f"âŒ Error chunking CSV: {e}")
        return False

def main():
    # Configuration
    input_csv = "../Semantic_Recent.csv"  # The renamed file
    chunk_size = 1000  # 1000 movies per chunk
    output_directory = "data_chunks"
    
    print("ğŸ”§ CSV Chunking Tool")
    print("=" * 50)
    
    success = chunk_csv_file(input_csv, chunk_size, output_directory)
    
    if success:
        print(f"\nâœ¨ Success! CSV has been chunked into {output_directory}/")
        print("\nğŸ“ Next steps:")
        print("1. The database will automatically load from chunks")
        print("2. Each chunk contains ~1000 movies for optimal performance") 
        print("3. VS Code will no longer have issues with large files")
        print("4. Runtime memory usage will be much lower")
    else:
        print("\nğŸ’¥ Failed to chunk CSV file!")

if __name__ == "__main__":
    main()